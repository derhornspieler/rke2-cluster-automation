# Optional override for the namespace where chart resources are created.
# Leave empty to use the Helm release namespace passed via `-n/--namespace`.
namespaceOverride: ""

# ------------------------------------------------------------------
# Global settings
# ------------------------------------------------------------------
replicaCounts:
  controlPlane: 3          # number of RKE2 control‑plane VMs
  worker: 1            # number of RKE2 worker VMs

resources:
  controlPlane:
    cpuCores: 4                 # initial CPU sockets/cores
    maxCpuSockets: 2            # optional ceiling for CPU hotplug (defaults to cpuCores)
    memoryMi: 16386             # initial guest memory in MiB
    maxMemoryMi: 16386          # optional ceiling for memory hotplug (defaults to memoryMi)
    enableHotplug: false        # add harvesterhci.io/enableCPUAndMemoryHotplug
  worker:
    cpuCores: 4
    maxCpuSockets: 2
    memoryMi: 16386
    maxMemoryMi: 16386
    enableHotplug: false

image:
  namespace: harvester-public        # Namespace that holds the VirtualMachineImage
  name: rocky-9-cloudimg             # Must exist in Harvester’s image catalog
  pullPolicy: IfNotPresent

vmNamePrefix: rke2                   # Prefix for generated VM names (defaults to Helm release name)

controlPlane:
  allowWorkloads: true               # If false, taint control planes with NoSchedule to keep user workloads off them

storage:
  className: ""                     # leave empty to use Harvester's per-image storage class
  accessMode: ReadWriteMany
  volumeMode: Block
  size: 30Gi
  controlPlaneSize: 100Gi
  workerSize: 150Gi

networks:
  vm:
    namespace: hvst-mgmt
    name: vlan2003                   # Workload VM network (VLAN 2003)
    interface: eth0                  # Guest interface name used in networkData
    prefix: 24                       # CIDR prefix length for static IPs
    gateway: ""                      # Optional gateway inserted into networkData
    nameservers: []                  # Optional DNS servers for static IPs
    dhcp:
      controlPlane: false
      worker: false
    macAddresses:
      controlPlane: []               # Optional list of MACs per control-plane VM
      worker: []                     # Optional list of MACs per worker VM
    staticIPs:
      controlPlane: []               # Optional static IPv4 list per control-plane VM
      worker: []                     # Optional static IPv4 list per worker VM
  rancher:
    enabled: false                  # Set true to attach a second interface dedicated to Rancher
    namespace: ""                  # Namespace of the Rancher NetworkAttachmentDefinition
    name: ""                       # Name of the Rancher NetworkAttachmentDefinition
    interface: eth1                 # Guest interface name for the secondary NIC
    prefix: 24                      # CIDR prefix for static IPs on the Rancher NIC
    routes: []                      # Optional list of extra routes (to/24 via/scope) for the Rancher NIC
    staticIPs:
      controlPlane: []              # Static IPv4s for control-plane VMs on the Rancher NIC
      worker: []                    # Static IPv4s for worker VMs on the Rancher NIC

# Legacy Harvester LoadBalancer values (unused when kubeVip.enabled=true)
loadBalancer:
  enabled: false
  namespace: default
  vip: ""
  subnet: ""
  gateway: ""
  network: ""

kubeVip:
  enabled: false
  namespace: kube-system
  useDaemonSet: true
  vip_address: ""                    # VIP address
  vip_interface: eth0                # Interface for the VIP
  vip_cidr: ""                       # Mask length for the VIP (e.g. 24)
  imageRepository: ghcr.io/kube-vip/kube-vip
  imageTag: v1.0.2
  imagePullPolicy: IfNotPresent
  imagePullSecrets: []
  ttl: 30
  arp: true
  leaderElection: true
  cpEnabled: true
  svc_enabled: false
  lb_enabled: true
  lb_port: "6443"
  serviceAccountName: kube-vip-static       # ServiceAccount used by the static pod
  rbac:
    create: true                     # Set false if you manage kube-vip RBAC yourself
  securityContext:
    capabilities:
      add:
        - NET_ADMIN
        - NET_RAW
        - SYS_TIME
  hostNetwork: true                  # Set false if you manage kube-vip RBAC yourself

tlsSANs:
  - internal-vip.local

ssh:
  user: rocky                     # user that cloud‑init will create
  publicKey: "ssh-ed25519 "  # fill with your SSH public key (optional)

rke2:
  token: ""  # will be filled from secret rke2-token
  version: "v1.33.5+rke2r1"        # RKE2 version you want to run
  cni: "canal"                     # or calico, multus, etc.

# ------------------------------------------------------------------
# Airgap / offline deployment overrides
# ------------------------------------------------------------------
# Leave values empty ("") to use public internet defaults.
# Only set the fields you need to override for your environment.
#
# NOTE: The following are already configurable via their own sections
# and do NOT need to be duplicated here:
#   kubeVip.imageRepository / kubeVip.imageTag  (kube-vip image)
#   vmDeployment.image                          (kubectl job image)
#   rancherManager.chartRepo                    (Rancher Helm repo)
#   metallb.chartRepo                           (MetalLB Helm repo)
#   certManager.chartRepo                       (cert-manager Helm repo)
airgap:
  # --------------- Private CA Certificate ---------------
  # PEM-encoded CA certificate for trusting private registries and
  # internal services. Written to /etc/pki/ca-trust/source/anchors/
  # on each VM and added to the system trust store via update-ca-trust.
  # Also referenced in registries.yaml tls.ca_file if registries are configured.
  # Paste the full PEM block including BEGIN/END lines.
  privateCA: ""

  # --------------- RKE2 Private Registry (registries.yaml) ---------------
  # Written to /etc/rancher/rke2/registries.yaml on each VM before RKE2 starts.
  # Supports HTTPS registries, OCI registries, mirror endpoints, auth, and TLS.
  # See: https://docs.rke2.io/install/containerd_registry_configuration
  registries:
    # Mirror configuration: map public registries to private endpoints.
    # Example:
    #   mirrors:
    #     docker.io:
    #       endpoint:
    #         - "https://registry.internal:5000"
    #     ghcr.io:
    #       endpoint:
    #         - "https://registry.internal:5000"
    #     "registry.k8s.io":
    #       endpoint:
    #         - "https://registry.internal:5000"
    mirrors: {}

    # Per-registry auth and TLS configuration.
    # When privateCA is set, ca_file is auto-populated; you can also set it explicitly.
    # Example:
    #   configs:
    #     "registry.internal:5000":
    #       auth:
    #         username: admin
    #         password: secret
    #       tls:
    #         insecure_skip_verify: false
    configs: {}

  # --------------- RKE2 System Default Registry ---------------
  # Prepended to all RKE2 system images (pause, etcd, kube-apiserver, etc.)
  # Example: "registry.internal:5000"
  systemDefaultRegistry: ""

  # --------------- RKE2 Installation ---------------
  # Override the RKE2 install script URL. Default: https://get.rke2.io
  rke2InstallUrl: ""

  # Installation method: "rpm" (default on RHEL/Rocky) or "tar" (for full airgap).
  # When "tar", set rke2ArtifactPath to the directory containing pre-downloaded tarballs.
  rke2InstallMethod: ""

  # Local path on the VM to pre-staged RKE2 artifacts (tarballs + images).
  # Used with rke2InstallMethod: "tar".
  # The artifacts must be pre-loaded onto the VM (e.g. via cloud-init write_files
  # or a shared filesystem). Default location if empty: not set.
  rke2ArtifactPath: ""

  # --------------- Package Repositories (yum/dnf) ---------------
  # Custom yum/dnf repo definitions written to /etc/yum.repos.d/ on each VM.
  # Use for local mirrors of Rocky Linux BaseOS/AppStream/Extras and
  # RKE2 RPM repos (rke2-common, rke2-<version>, rke2-selinux).
  #
  # Each entry creates a repo file: /etc/yum.repos.d/<key>.repo
  # Example:
  #   yumRepos:
  #     local-baseos:
  #       name: "Local BaseOS Mirror"
  #       baseurl: "http://repo.internal/rocky/9/BaseOS/x86_64/os/"
  #       gpgcheck: false
  #     local-appstream:
  #       name: "Local AppStream Mirror"
  #       baseurl: "http://repo.internal/rocky/9/AppStream/x86_64/os/"
  #       gpgcheck: false
  #     rke2-common:
  #       name: "RKE2 Common"
  #       baseurl: "http://repo.internal/rke2/latest/common/centos/9/noarch"
  #       gpgcheck: false
  #     rke2-version:
  #       name: "RKE2 v1.33"
  #       baseurl: "http://repo.internal/rke2/latest/1.33/centos/9/x86_64"
  #       gpgcheck: false
  yumRepos: {}

  # Disable default Rocky Linux repos (rocky.repo, rocky-extras.repo, etc.)
  # Set to true for fully airgapped environments that use only custom yumRepos above.
  disableDefaultRepos: false

  # --------------- Container Images ---------------
  # Override images used by deployment tooling (outside RKE2's image management).
  images:
    # kube-vip init container, kube-vip DaemonSet patch, and CSI smoke test.
    # Default: busybox:1.36
    busybox: ""

    # Bootstrap job that extracts kubeconfig from the control-plane VM.
    # Default: docker.io/library/alpine:3.20
    bootstrap: ""

  # --------------- OS Images ---------------
  # Override Harvester VirtualMachineImage download URLs.
  osImages:
    # Default: https://cloud-images.ubuntu.com/releases/22.04/release/ubuntu-22.04-server-cloudimg-amd64.img
    ubuntu: ""
    # Default: https://dl.rockylinux.org/pub/rocky/9/images/x86_64/Rocky-9-GenericCloud-Base.latest.x86_64.qcow2
    rocky9: ""

  # --------------- CSI Snapshot CRDs ---------------
  # Override URLs for CSI snapshot CRD manifests (applied on the guest cluster).
  # Accepts URLs or local file paths.
  csiSnapshotCrdUrls:
    # Default: https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.3.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
    volumeSnapshotClasses: ""
    # Default: https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.3.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
    volumeSnapshotContents: ""
    # Default: https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.3.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
    volumeSnapshots: ""

# ------------------------------------------------------------------
# Cilium CNI + L2 LoadBalancer
# ------------------------------------------------------------------
# When enabled, set rke2.cni to "cilium" and Cilium replaces both
# Canal (CNI) and MetalLB (L2 LoadBalancer). Uses HelmChartConfig
# to customize the RKE2-bundled Cilium chart.
cilium:
  enabled: false
  # L2 announcement configuration (replaces MetalLB)
  l2:
    enabled: false
    # IP address pools for L2 announcements (CIDR notation)
    addressPools: []
    # Example:
    #   - name: rancher-vip
    #     addresses:
    #       - 172.16.3.6/32
    #     autoAssign: false
    #   - name: general
    #     addresses:
    #       - 172.16.3.7/30
    #     autoAssign: true
  # Extra Helm values passed to the RKE2-bundled Cilium chart via HelmChartConfig
  values: {}

# ------------------------------------------------------------------
# Traefik Gateway Controller
# ------------------------------------------------------------------
# Deploys Traefik as a Gateway API controller via HelmChart CRD.
# Replaces rke2-ingress-nginx. Serves as the LoadBalancer entry point.
traefik:
  enabled: false
  chartRepo: https://traefik.github.io/charts
  chartName: traefik
  chartVersion: ""
  helmChartNamespace: kube-system
  namespace: traefik-system
  service:
    type: LoadBalancer
    loadBalancerIP: ""            # Static LB IP (from Cilium L2 pool)
    annotations: {}
  # Extra Helm values for Traefik
  values: {}

# ------------------------------------------------------------------
# Gateway API
# ------------------------------------------------------------------
# Installs Gateway API CRDs and creates Gateway/HTTPRoute resources
# for Rancher. Works with cert-manager gateway-shim for automatic TLS.
gatewayAPI:
  enabled: false
  # Gateway API CRD version to install
  crdVersion: "v1.2.1"
  # Override CRD bundle URL (for airgap)
  crdUrl: ""
  # Gateway resource for Rancher
  gateway:
    name: rancher-gateway
    namespace: cattle-system
    listenerPort: 443
    hostname: ""                  # e.g. rancher.aegisgroup.ch
    # cert-manager gateway-shim annotations
    certIssuerName: ""            # ClusterIssuer name for auto-TLS
    certIssuerKind: ClusterIssuer

rancherManager:
  enabled: false
  chartRepo: https://releases.rancher.com/server-charts/latest
  chartName: rancher
  chartVersion: ""
  helmChartNamespace: kube-system
  namespace: cattle-system
  hostname: ""
  bootstrapPassword: ""
  replicas: 3
  # Pod resource requests/limits for HPA utilization metrics
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "2"
      memory: "4Gi"
  rancherManager:
    extraEnv:
      - name: CATTLE_SERVER_URL
        value: https://rancher.yourdomain.com
  service:
    type: LoadBalancer
    loadBalancerIP: ""
    annotations: {}
  ingress:
    tlsSource: secret           # rancher (default self-signed), letsEncrypt, or secret/private-ca
    tlsSecretName: ""          # Required when tlsSource=secret
    tlsSecret:                 # Optional inline TLS secret material (PEM); requires tlsSource=secret
      create: false
      certificate: ""          # Include PEM content, e.g. -----BEGIN CERTIFICATE-----
      privateKey: ""           # Include PEM content, e.g. -----BEGIN PRIVATE KEY-----
    extraAnnotations: {}

# ------------------------------------------------------------------
# CloudNativePG — External PostgreSQL for Rancher HA + HPA
# ------------------------------------------------------------------
# Deploys the CloudNativePG operator and a PostgreSQL Cluster for use
# as Rancher's external database backend. Enables stateless Rancher
# pods so HPA can scale them dynamically.
cloudNativePG:
  enabled: false

  # --- CNPG Operator (Helm chart) ---
  chartRepo: https://cloudnative-pg.github.io/charts
  chartName: cloudnative-pg
  chartVersion: ""                    # Pin a version, e.g. "0.23.0"
  helmChartNamespace: kube-system     # Where the HelmChart CRD lives
  namespace: cnpg-system              # Where the operator pods run
  values: {}                          # Extra Helm values for the operator

  # --- PostgreSQL Cluster ---
  cluster:
    name: rancher-postgres
    namespace: cattle-system           # Same namespace as Rancher
    instances: 3                       # 1 primary + 2 replicas
    database: rancher
    owner: rancher                     # PostgreSQL user/role
    storage:
      size: 20Gi
      storageClass: ""                 # Leave empty for default (harvester)
    postgresql:                        # postgresql.conf parameters
      max_connections: "200"
      shared_buffers: "256MB"
    resources:                         # Pod resource requests/limits
      requests:
        cpu: "250m"
        memory: "512Mi"
      limits:
        cpu: "1"
        memory: "1Gi"

  # --- Database credentials ---
  # Password for the PostgreSQL "rancher" user.
  # If empty, CNPG auto-generates credentials in a Secret named
  # "<cluster.name>-app" (e.g. rancher-postgres-app).
  password: ""

# ------------------------------------------------------------------
# Rancher HPA (Horizontal Pod Autoscaler)
# ------------------------------------------------------------------
# Requires cloudNativePG.enabled=true for stateless Rancher pods.
rancherHPA:
  enabled: false
  minReplicas: 3
  maxReplicas: 7
  metrics:
    cpu:
      averageUtilization: 70
    memory:
      averageUtilization: 80

metallb:
  enabled: false
  chartRepo: https://metallb.github.io/metallb
  chartName: metallb
  chartVersion: ""
  helmChartNamespace: kube-system
  namespace: metallb-system
  valuesContent: ""
  values: {}
  addressPools: []              # e.g. [{ name: rancher, protocol: layer2, autoAssign: false, addresses: ["192.168.10.6-192.168.10.9"] }]
    # - name: rancher-vip
    #   protocol: layer2
    #   autoAssign: false
    #   addresses:
    #     - 172.16.3.6-172.16.3.6   # dedicated IP for Rancher
certManager:
  enabled: false
  chartRepo: https://charts.jetstack.io
  chartName: cert-manager
  chartVersion: ""
  helmChartNamespace: kube-system
  namespace: cert-manager
  installCRDs: true
  valuesContent: ""
  values: {}
  certificate:
    create: false
    secretName: ""
    commonName: ""
    dnsNames: []
    issuerRef:
      name: ""
      kind: ClusterIssuer
      group: cert-manager.io
  ca:
    create: false
    secretName: ""
    commonName: ""
    issuerName: ""
    selfSignedIssuerName: ""
    certificateName: ""

cloudProvider:
  cloudConfig: ""              # Inline kubeconfig for Harvester CCM (optional)
  # cloudConfig: |
    # ########## cloud config ############
    # apiVersion: v1
    # clusters:
    # - cluster:
    #     insecure-skip-tls-verify: true
    #     server: https://172.16.2.2/k8s/clusters/local
    #   name: local
    # contexts:
    # - context:
    #     cluster: local
    #     namespace: rke2
    #     user: rke2-mgmt-cloud-provider-rke2-local
    #   name: rke2-mgmt-cloud-provider-rke2-local
    # current-context: rke2-mgmt-cloud-provider-rke2-local
    # kind: Config
    # users:
    # - name: rke2-mgmt-cloud-provider-rke2-local
    #   user:
    #     token: <token>
  configPath: /var/lib/rancher/rke2/etc/config-files/cloud-provider-config

vmDeployment:
  image: alpine/kubectl:1.34.2
  backoffLimit: 3

harvesterTemplates:
  enabled: true
